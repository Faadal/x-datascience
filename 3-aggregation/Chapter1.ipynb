{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHAPTER 1 - INTRODUCTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine learning setting**\n",
    "\n",
    "A space of objects $\\mathcal{X}$.\n",
    "A set of labels, usually $\\mathbb{R}$ or a subset of $\\mathbb{R}$ like $\\{-1,+1\\}$, e.g in classification problems.\n",
    "\n",
    "A set of functions $\\mathcal{F}$ called predictors.\n",
    "For each $f\\in\\mathcal{F}$, $f:\\mathcal{X}\\rightarrow\\mathbb{R}$.\n",
    "_Idea_: $f(x)$ is meant to predict the label $y$ associated to an object $x$.\n",
    "\n",
    "A loss function $\\ell:\\mathbb{R}^2 \\rightarrow \\mathbb{R}_{+} $ with $\\ell(y,y)=0$ for any $y$. _Idea_: $\\ell(f(x),y)$ measures the distance between the prediction $f(x)$ and the actual label $y$. _Recurring examples_: quadratic loss (for regression) $\\ell(u,v)=(u-v)^2$, absolute loss $\\ell(u,v)=|u-v|$ and the zero-one loss for classification $\\ell(u,v)=\\mathbf{1}(u\\neq v)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch learning**\n",
    "\n",
    "A sample $[(X_i,Y_i)]_{1\\leq i\\leq n}$ i.i.d from a probability distribution $P$ over $\\mathcal{X}\\times\\mathbb{R}$ equiped with a suitable $\\sigma$-algebra. We measure the quality of a predictor $f$ by its out-of-sample prediction risk $R(f)=\\mathbb{E}_{(X,Y)\\sim P}[\\ell(Y,f(X))]$. In order to do so, note that measurability assumptions are necessary (every $f\\in\\mathcal{F}$ is measurable etc.). In this case, we want to build a (measurable) function $\\hat{f}:\\mathcal{X}\\rightarrow\\mathbb{R} $ that such that $R(\\hat{f})$ is as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Online learning**\n",
    "\n",
    "A sequence $(x_i,y_i)_{i \\geq 1}\\in (\\mathcal{X}\\times \\mathbb{R})^{\\mathbb{N}^*}$. No assumption on how this sequence is generated. It can be random, or deterministic. It can also be generated by a malevolent and omniscious oponent who is able to anticipate your strategy. Elements are revealed sequentially: at each time step $t\\in\\mathbb{N}^*$, $x_t$ is given, one has to predict $y_t$. We use the notation $\\hat{y}_t$ for the prediction. Then, $y_t$ is revealed and one incurs the loss $\\ell(y_t,\\hat{y}_t)$. For short, we will often write $\\ell_t=\\ell(y_t,\\hat{y}_t)$.  After $T$ steps, the cumulated loss is $$L_T = \\sum_{t=1}^T \\ell_t = \\sum_{t=1}^T \\ell(y_t,\\hat{y}_t) .$$\n",
    "The objective is to keep this as small as possible for each $T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What can we expect? Two different points of view**\n",
    "\n",
    "Consider online learning where the labels $y_t\\in\\{-1,+1\\}$ and use the zero-one loss. Consider a malevolent and omniscient oponent: at each step, the opponent produces $x_t$, you perform a prediction $\\hat{y}_t$ and the oponent then outputs $y_t = -\\hat{y}_t$. First, note that this problem seems hopeless. Then, note that $\\forall T\\in\\mathbb{N}^*,\\quad L_T = T $, so, in a not-so-hopeless problem, we really would like to have $L_T = o(T)$.\n",
    "\n",
    "Generally, there are two points of view in such a situation:\n",
    "\n",
    "1) make assumpions in order to make the problem easier. That is, impose some constraint on the way the data is produced. In online learning, it would be for example the so-called _realizable case_ where we assume that there is actually a function $f_0\\in\\mathcal{F}$ such that for all $t$, $f_{0}(x_t)=y_t$ (so the opponent is not free to chose $y_t$). Note that this point of view is often unrealistic.\n",
    "\n",
    "2) don't make any assumption, but relax your objective instead. For example, instead of trying to do the best possible prediction, just try to as well as the best predictor in $\\mathcal{F}$. This is the _aggregation_ point of view. So, instead of $L_T$, we want to control\n",
    "$$ \\mathcal{R}_T(\\mathcal{F}) = \\sum_{t=1}^T \\ell(y_t,\\hat{y}_t) - \\inf_{f\\in\\mathcal{F}} \\sum_{t=1}^T \\ell(y_t,f(x_t)) = \\sum_{t=1}^T \\ell_t - \\inf_{f\\in\\mathcal{F}} \\sum_{t=1}^T \\ell(y_t,f(x_t)) .$$\n",
    "This quantity is called the regret at time $T$.\n",
    "\n",
    "Basically, it is possible in 1) to propose predictions $\\hat{y}_t$ with $L_T = o(T)$ (under suitable assumptions) and in 2) to propose predictions $\\hat{y}_t$ with $ \\mathcal{R}_T(\\mathcal{F}) = o(T)$ (under no assumptions at all!). Note that it is a more general approach, in the sense that if we have a prediction strategy such that $ \\mathcal{R}_T(\\mathcal{F}) = o(T)$, then we can focus on the realizable case and we obtain $\\sum_{t=1}^T \\ell_t \\leq \\inf_{f\\in\\mathcal{F}} \\sum_{t=1}^T \\ell(y_t,f(x_t)) + o(T) = \\sum_{t=1}^T \\ell(y_t,f_0(x_t)) + o(T) = o(T)$. We will mainly focus on 2) in the next chapters, however, we end this introduction by two examples of prediction strategies in the realizable case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The realizable case: ${\\rm consistent}$ algorithm**\n",
    "\n",
    "Context: $(x_i,y_i)_{i \\geq 1}\\in (\\mathcal{X}\\times \\{-1,1\\})^{\\mathbb{N}^*}$, a _finite_ set of functions $\\mathcal{F}=\\{f_1,\\dots,f_M\\}$ and $\\exists i_0\\in\\{1,\\dots,M\\},\\forall t\\in\\mathbb{N}^*, f_{i_0}(x_t)=y_t$. We use the zero-one loss.\n",
    "\n",
    "Initialization: $V_1 = \\{1,\\dots,M\\}$, $i(1)=\\min(V_1)=1$.\n",
    "\n",
    "Step $t$: $\\hat{y}_t = f_{i(t)}(x_t)$. Once $y_t$ is revealed, set $V_{t+1} = \\{i\\in V_{t}: f_i(x_t) = y_t\\}$ and $i(t+1)=\\min(V_{t+1})$.\n",
    "\n",
    "_Theorem_: $L_T \\leq M-1$.\n",
    "\n",
    "_Proof_: note that $\\ell_t=1 \\Leftrightarrow f_{i(t)}(x_t) \\neq y_t \\Rightarrow |V_{t+1}| \\leq |V_t| -1 $ that is $\\ell_t = 1\\leq |V_{t}|-|V_{t+1}|$. Moreover when $\\ell_t=0$ we always have $\\ell_t=0\\leq |V_{t}|-|V_{t+1}|$. So\n",
    "$$ \\sum_{t=1}^T \\ell_t \\leq \\sum_{t=1}^T ( |V_t|-|V_{t+1}|) =  |V_1| - |V_{T+1}| = M-|V_{T+1}|\\leq M-1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The realizable case: ${\\rm halving}$ algorithm**\n",
    "\n",
    "Same context, but _much clever_: at each step, make all the predictors in $V(t)$ vote.\n",
    "\n",
    "Initialization: $V_1 = \\{1,\\dots,M\\}$.\n",
    "\n",
    "Step $t$: $\\hat{y}_t = {\\rm sign}\\left(\\sum_{i\\in V(t)} f_i(x_t)\\right)$. By convention: ${\\rm sign}(x) = -1$ if $x<0$ and $+1$ if $x>0$. Once $y_t$ is revealed, set $V_{t+1} = \\{i\\in V_{t}: f_i(x_t) = y_t\\}$.\n",
    "\n",
    "_Theorem_: $L_T \\leq \\log_2(M)$.\n",
    "\n",
    "_Proof_: in the same way $\\ell_t = 1 \\Rightarrow |V_{t+1}| \\leq \\frac{|V_t|}{2}$ that is $\\ell_t = 1 \\leq \\log_2\\left(\\frac{|V_t|}{|V_{t+1}|}\\right)$ so\n",
    "$$ \\sum_{t=1}^T \\ell_t \\leq \\sum_{t=1}^T \\log_2\\left(\\frac{|V_t|}{|V_{t+1}|}\\right) =  \\log_2\\left(\\frac{|V_1|}{|V_{T+1}|}\\right) \\leq \\log_2(M).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Illustration**\n",
    "\n",
    "Toy example: $\\mathcal{X}=\\{0,\\dots,N-1\\}$, $\\mathcal{F}=(f_0,\\dots,f_{M-1})$ where the $f_i$ will be drawn i.i.d from the set of all functions $\\{0,\\dots,N-1\\}\\rightarrow\\{-1,+1\\}$.\n",
    "\n",
    "You choose $M$ and $N$, and the time horizon $T$. The true function $i_0$ is chosen randomly, as well as the inputs $x_1,x_2,\\dots$ Don't hesitate to play with $M$, $N$, $T$ as well as with the scale of the plots below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "M = 100000\n",
    "T = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the errors of ${\\rm consistent}$ and ${\\rm halving}$ over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zapfack/tools/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/zapfack/tools/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: DeprecationWarning: non integer (and non boolean) array-likes will not be accepted as indices in the future\n",
      "/home/zapfack/tools/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:14: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEOCAYAAACTqoDjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/h8i7m+EueP1NQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc69870ce10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pylab import *\n",
    "from numpy import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = floor(rand(T)*N)\n",
    "f = sign(randn(M,N))\n",
    "iO = floor(rand()*M)\n",
    "y = f[iO,[x]][0]\n",
    "\n",
    "consistent,halving = [],[]\n",
    "V = arange(M)\n",
    "for t in range(T):\n",
    "    u = f[[V],x[t]][0]\n",
    "    list.append(consistent,(1-u[0]*y[t])/2)\n",
    "    list.append(halving,(1-sign(sum(u)+0.1)*y[t])/2)\n",
    "    V = V[u==y[t]]\n",
    "\n",
    "consistent = cumsum(consistent)\n",
    "halving = cumsum(halving)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "plt.ylim(0,1.2*log(M)/log(2))\n",
    "l1, = plt.plot(consistent)\n",
    "l2, = plt.plot(halving)\n",
    "l3, = plt.plot(repeat(log(M)/log(2),T))\n",
    "l4, = plt.plot(repeat(M-1,T))\n",
    "plt.legend([l1,l2,l3,l4],['consistent','halving','$\\log_2(M)$','$M-1$'])\n",
    "plt.grid(True,which=\"both\")\n",
    "plt.xlabel(r\"Time step $t$\")\n",
    "plt.ylabel(r\"Number of errors $L_t$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical sets of predictors**\n",
    "\n",
    "When we have a set of functions $\\mathcal{S}=\\{f_1,\\dots,f_M\\}$, to compete against the best member of $\\mathcal{S}$, that is, setting $\\mathcal{F}=\\mathcal{S}$ is often called MS-type aggregation (MS stands for Model Selection). To set $\\mathcal{F}={\\rm conv}(\\mathcal{S}) = \\left\\{\\sum_{i=1}^n \\alpha_i f_i(\\cdot),\\alpha_i\\geq 0, \\sum_{i=1}^n \\alpha_i =1\\right\\}$ leads to C-type aggregation (C stands for Convex). Finally, $\\mathcal{F}={\\rm span}(\\mathcal{S}) = \\left\\{\\sum_{i=1}^n \\alpha_i f_i(\\cdot),\\alpha\\in\\mathbb{R}^M\\right\\}$ leads to L-type aggregation (L stands for Linear).\n",
    "\n",
    "When $\\mathcal{X}=\\mathbb{R}^M$, we can think of $f_1(x)=x_1,\\dots,f_M(x)=x_M$ for $x=(x_1,\\dots,x_p)$. But anything else works too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "This chapter is based on Section 1 in\n",
    "\n",
    "S. Shalev-Shwartz, Online Learning and Online Convex Optimization, _Foundations and Trends in Machine Learning_, 4(2), pp. 107-194 (2011).\n",
    "\n",
    "In the batch setting, the analogous of the realizable case 1) would be to assume a parametric model. Then, theory is easier, but this assumption is most of the time simply wrong. But this is the only way that what was taught in universities in the 90s! On the other hand, in practice, people focused on effective classification and regression algorithms, and that worked! Leo Breiman attracted academics attention to this issue in his fascinating paper:\n",
    "\n",
    "L. Breiman, Statistical Modelling: the two Cultures, _Statistical Science_, 16(3), pp. 199-231 (2001).\n",
    "\n",
    "There is actually an analogous of 2) in the batch setting: statistical learning theory, that started in the early 70's, with the works of Vapnik and Chervonenkis. Aggregation theory is a part of statistical learning theory. It shows that good learning algorithms try to mimic the best predictor in an adequate family $\\mathcal{F}$. In theory, that means that we are able to control $R(\\hat{f})-\\inf_{f\\in\\mathcal{F}}R(f)$. We will see some aspects of this theory in the next chapters. Note that the MS, C and L terminology is due to one of the founding works in aggregation theory in statistics:\n",
    "\n",
    "A. Nemirovski, Topics in Non-parametric Statistics, in _Ecole d'Eté de Probabilités de Saint-Flour XXVIII_, Lecture Notes in Mathematics, Springer (1998)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
