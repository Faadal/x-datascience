{
 "metadata": {
  "name": "",
  "signature": "sha256:308dde0c7991908b45943fbf7e9d66a618eda690faaecf981fe6393da6939996"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**CHAPTER 4 - PAC-BAYESIAN BOUNDS FOR BATCH LEARNING**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Notations**\n",
      "\n",
      "Idea of this chapter: in the batch setting, is is possible in some situations to avoid online algorithms and to use the EWA on the whole sample.\n",
      "\n",
      "The notations are similar to Chapter 3, but in the batch setting.\n",
      "Fix $f_1,\\dots,f_M$ and $f(\\cdot)=(f_1(\\cdot),\\dots,f_M(\\cdot))$. Sample $(X_1,Y_1),\\dots,(X_n,Y_n)$ i.i.d from a proba. distribution $P$, values in $\\mathcal{X}\\times [-B_y,B_y]$. For any $\\alpha\\in\\mathbb{R}^M$ we put $f_{\\alpha}(\\cdot) = [\\left<\\alpha,f(\\cdot)\\right>]_B $ with $B\\geq B_y$. We put $\\mathcal{F}=\\{f_\\alpha,\\alpha\\in\\mathbb{R}^M\\}$. Finally, any loss function $\\ell$ such that $\\forall y\\in[-B_y,B_y]$, $\\ell(y,\\cdot)$ is upper bounded by $C$. Remind that $R(f)=\\mathbb{E}_{(X,Y)\\sim P}[\\ell(Y,f(X))]$ and we introduce the _empirical risk_:\n",
      "$$r_n(f) = \\frac{1}{n}\\sum_{i=1}^n [\\ell(Y_i,f(X_i)) $$\n",
      "that, on the contrary to $R(f)$, can be computed based on the sample only."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**A PAC-Bayesian bound for batch learning**\n",
      "\n",
      "As in the previous chapter we fix $\\lambda>0$, an initial distribution $\\pi$ (usually refered to as _the prior_) and define\n",
      "$$\n",
      "\\hat{\\rho}_{\\lambda}({\\rm d}\\theta) = \\frac{\\exp[-\\lambda r_n(f_\\theta)]\\pi({\\rm d}\\theta)}{\\int \\exp[-\\lambda r_n(f_\\alpha)]\\pi({\\rm d}\\alpha)}.\n",
      "$$\n",
      "\n",
      "_Theorem_: for any $\\varepsilon\\in(0,1)$, with probability at least $1-\\varepsilon$ on the drawing of the sample we have simultaneously\n",
      "$$\n",
      "\\int R(f_\\theta) \\hat{\\rho}_{\\lambda}({\\rm d}\\theta)\n",
      "\\leq\n",
      "\\inf_{\\rho}\n",
      "\\left\\{\n",
      "\\int r_n(f_\\theta) \\rho({\\rm d}\\theta) + \\frac{\\lambda C^2}{8n} + \\frac{\\mathcal{K}(\\rho,\\pi)+\\log\\left(\\frac{2}{\\varepsilon}\\right)}{\\lambda}\n",
      "\\right\\}\n",
      "$$\n",
      "and\n",
      "$$\n",
      "\\int R(f_\\theta) \\hat{\\rho}_{\\lambda}({\\rm d}\\theta)\n",
      "\\leq\n",
      "\\inf_{\\rho}\n",
      "\\left\\{\n",
      "\\int R(f_\\theta) \\rho({\\rm d}\\theta) + \\frac{\\lambda C^2}{4n} + 2\\frac{\\mathcal{K}(\\rho,\\pi)+\\log\\left(\\frac{2}{\\varepsilon}\\right)}{\\lambda}\n",
      "\\right\\}.\n",
      "$$\n",
      "\n",
      "_Proof_: done in class, and in the references (Catoni's book). Use Hoeffding's inequality and the lemma of Chapter 3.\n",
      "\n",
      "_Remark_: the first bound depends on the sample, so you can compute it in practice, it gives you a numerical guarantee on the out of sample prediction. The second one is more theoretical, it gives you a rate of convergence to the best out-of-sample prediction. For example, using calculations from Chapter 3, in the MS-type aggregation, with $\\lambda = 2\\sqrt{2n\\log(M)}$ and $\\pi$ uniform on $\\{e_1,\\dots,e_M\\}$ the bounds become:\n",
      "$$\n",
      "\\int R(f_\\theta) \\hat{\\rho}_{\\lambda}({\\rm d}\\theta)\n",
      "\\leq\n",
      "\\inf_{1\\leq i\\leq M} r_n(f_i) + \\sqrt{\\frac{\\log(M)}{2n}} + \\frac{\\log\\left(\\frac{2}{\\varepsilon}\\right)}{2\\sqrt{2n\\log(M)}}\n",
      "$$\n",
      "and\n",
      "$$\n",
      "\\int R(f_\\theta) \\hat{\\rho}_{\\lambda}({\\rm d}\\theta)\n",
      "\\leq\n",
      "\\inf_{1\\leq i\\leq M} R(f_i) + \\sqrt{\\frac{2\\log(M)}{n}} + \\frac{\\log\\left(\\frac{2}{\\varepsilon}\\right)}{\\sqrt{2n\\log(M)}}.\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Improved bound under margin assumption**\n",
      "\n",
      "Question: can we reach rates in $\\frac{1}{n}$? The answer is yes, under additional assumptions. Here: margin assumption. First, define $\\bar{f}$ as a minimizer of $R(\\cdot)$ on $\\mathcal{F}$ (when there is at least one). Then we assume that, for any $f\\in\\mathcal{F}$,\n",
      "$$ \\mathbb{E}_{(X,Y)\\sim P}\\left\\{\\left[\\ell(Y,f(X))-\\ell(Y,\\bar{f}(X))\\right]^2\\right\\}\n",
      "      \\leq c [R(f) - R(\\bar{f})].$$\n",
      "_Note that this assumption is restrictive_. For example, in bounded regression with square loss, using Pythagorean theorem you can check that it is satisfied when the regression function $\\mathbb{E}_{(X,Y)\\sim P}(Y|X=x)$ actually belongs to $\\mathcal{F}$ - it sounds almost as unrealistic as the \"realizable case\"!\n",
      "\n",
      "_Theorem_: for any $\\varepsilon\\in(0,1)$, with probability at least $1-\\varepsilon$ on the drawing of the sample we have\n",
      "$$\n",
      "\\int R(f_\\theta) \\hat{\\rho}_{\\lambda}({\\rm d}\\theta)\n",
      "\\leq R(\\bar{f})\n",
      "+\n",
      "\\frac{1+\\frac{\\lambda c}{n}g\\left(\\frac{2\\lambda C}{n}\\right)}\n",
      "{1-\\frac{\\lambda c}{n}g\\left(\\frac{2\\lambda C}{n}\\right)}\n",
      "\\inf_{\\rho}\n",
      "\\left\\{\n",
      "\\int (R(f_\\theta)-R(\\bar{f})) \\rho({\\rm d}\\theta) + 2\\frac{\\mathcal{K}(\\rho,\\pi)+\\log\\left(\\frac{2}{\\varepsilon}\\right)}{\\lambda}\n",
      "\\right\\}\n",
      "$$\n",
      "where $g$ is the Bernstein function $g(x)=\\frac{\\exp(x)-1-x}{x^2}$.\n",
      "\n",
      "_Proof_: Bernstein's inequality + lemma of Chapter 3 + margin assumption, see the references by Catoni.\n",
      "\n",
      "In case you don't like the big ratio in the right-hand side note that for example, using $g(1/2) \\leq 0.6$, the choice $\\lambda = \\frac{n}{4(c+C)} $ leads to\n",
      "$$\n",
      "\\int R(f_\\theta) \\hat{\\rho}_{\\lambda}({\\rm d}\\theta)\n",
      "\\leq R(\\bar{f})\n",
      "+\n",
      "1.36\n",
      "\\inf_{\\rho}\n",
      "\\left\\{\n",
      "\\int (R(f_\\theta)-R(\\bar{f})) \\rho({\\rm d}\\theta) + 8(c+C)\\frac{\\mathcal{K}(\\rho,\\pi)+\\log\\left(\\frac{2}{\\varepsilon}\\right)}{n}\n",
      "\\right\\}.\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**A sharp bound for regression with deterministic design**\n",
      "\n",
      "There is a very sharp bound in the case of regression with deterministic design: we observe $(x_1,Y_1),\\dots,(x_n,Y_n)$ where the $x_i$'s are deterministic and $Y_i = f^*(x_i) + \\varepsilon_i$ where the $\\epsilon_i$'s are i.i.d $\\mathcal{N}(0,\\sigma^2)$ for some known $\\sigma^2$ (it is possible to go beyond Gaussian noise, see the references). Of course $f^*$ is unknown and we want to do MS, C or L-type aggregation with $f_1,\\dots,f_M$ (without truncation). So we put $f(\\cdot)=(f_1(\\cdot),\\dots,f_M(\\cdot))$, $g_{\\alpha}(\\cdot)=\\left<\\alpha,f(\\cdot)\\right>$ and $\\mathcal{G}=\\left\\{g_\\alpha,\\alpha\\in\\mathbb{R}^M\\right\\}$. Here, we focus on a simpler objective: to reconstruct $f^*$ on the design points only. That is:\n",
      "$$ r_n(f) = \\frac{1}{n}\\sum_{i=1}^n (Y_i - f(x_i))^2 $$\n",
      "and\n",
      "$$ R(f) = \\frac{1}{n}\\sum_{i=1}^n (f^*(x_i) - f(x_i))^2 .$$\n",
      "As above we fix $\\lambda>0$, the prior $\\pi$ and put\n",
      "$$\n",
      "\\hat{\\rho}_{\\lambda}({\\rm d}\\theta) = \\frac{\\exp[-\\lambda r_n(g_\\theta)]\\pi({\\rm d}\\theta)}{\\int \\exp[-\\lambda r_n(g_\\alpha)]\\pi({\\rm d}\\alpha)}\n",
      "$$\n",
      "and\n",
      "$$\n",
      "\\hat{\\theta}_\\lambda = \\int \\theta \\hat{\\rho}_{\\lambda}({\\rm d}\\theta).\n",
      "$$\n",
      "\n",
      "_Theorem_: As soon as $0<\\lambda\\leq \\frac{n}{4\\sigma^2}$ we have\n",
      "$$\n",
      "\\mathbb{E}\\left[R(g_{\\hat{\\theta}_\\lambda})\\right]\n",
      "\\leq\n",
      "\\inf_{\\rho}\\left\\{\n",
      "\\int R(g_\\theta)\\rho({\\rm d}\\theta) + \\frac{\\mathcal{K}(\\rho,\\pi)}{\\lambda}\n",
      "\\right\\}.\n",
      "$$\n",
      "\n",
      "_Proof_: see the reference by Dalalyan & Tsybakov."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Conclusion of Chapters 3 and 4**\n",
      "\n",
      "EWA works well in the online setting, and through the online-to-batch trick it can be used in the batch setting under the name \"progressive mixture rule\". It is possible to avoid the online treatment in the batch setting, using the methods of Chapter 4 - let's call it batch-EWA. There is always a control of the risk (first theorem in Chapter 4) but it is usually not optimal; improvements are possible but only under very strong hypothesis.\n",
      "\n",
      "So, why not always use the online-EWA, even in the batch setting? Answer: in the general case, efficient algorithms for online-EWA are still not known. However, when we use the batch-EWA, there are good algorithms, this is the topic of the next chapter!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**References**\n",
      "\n",
      "This chapter is based on:\n",
      "\n",
      "O. Catoni, _Statistical Learning Theory and Stochastic Optimization_, Springer Lecture Notes in Mathematics vol. 1851, 2004.\n",
      "\n",
      "O. Catoni, _PAC-Bayesian Supervised Classification: the Thermodynamics of Statistical Learning_, IMS Lecture notes vol. 56, 2008.\n",
      "\n",
      "The seminal works on PAC-Bayesian bounds in the batch setting is due to McAllester, there are many links with bounds in information theory (Rissanen, Barron, etc...) see Catoni's books for exact references. The result for regression with deterministic design can be found in:\n",
      "\n",
      "A. Dalalyan & A. Tsybakov, Aggregation by Exponential Weighting, Sharp PAC-Bayesian Bounds and Sparsity, _Machine Learning_, 72(1-2), pp. 39-61, 2008."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}