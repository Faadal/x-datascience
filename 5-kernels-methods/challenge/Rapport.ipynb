{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repport Kernel methods : Digit Recognition Challenge\n",
    " <i> <u>Basile CALDERAN</u> & <u> Fabrice ZAPFACK </u> </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reading\n",
    "Data where given as 3 CSV files:\n",
    "<ul> \n",
    "<li> <b> Xtr.csv </b>  : Training images  </li> \n",
    "<li> **Ytr.csv** : Labels of Training images </li>\n",
    "<li> **Xte.csv** : Test images </li>\n",
    "</ul> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1 : Analysis and choices making\n",
    "During that first part we decided to use the different librairies available to quickly test different approaches to resolve this problem. The objective was to be able to quickly find the best models and their associated parameters.\n",
    "\n",
    "### Problem formulation\n",
    "The task given is a multiclass classification (10 classes). The dimension is high (784) so we have to keep in mind the **curse of dimensionnality**. <br/>\n",
    "\n",
    "The histogram on the training labels let us conclude that the problem was more or less <u> balanced </u> (no need to give different weigths to classes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "We have tried different preprocessing techniques in this part. We evaluated the impact of these operations on the classification task by comparing with crossvalidation score obtained using the raw data. The model used was the best one found in scikit learn classification models (see next section): \n",
    "<ul> \n",
    "<li> <b> Transpose </b> : Had no impact on classification (good for visaulition)  </li> \n",
    "<li> **Binarization** : It was used as asimple segmentation step. It happens to improve a little bit accuracy </li>\n",
    "<li> **Rescaling** : Used to project the data in $[0,1]$ but decreased the accuray : $X_i = (X_i - min(X_i))/(max(X_i)-min(X_i))$ </li>\n",
    "<li> **noise filtering** : Done with a mean filter (slidding window), lead to a decrease of accuracy  </li> \n",
    "<li> **Edge detector** :Done with a sliding window. A couple of filters were tested (derivative, laplacian, edge, ...). It Increased accuracy with a cross template. </li>\n",
    "<li> **Gauss Filter** : Sliding window where coefficients have a real and an imaginary part (orthogonals). Decreased accuracy. </li>\n",
    "<li> **PCA** : Good for dimension reduction. The best score is obtained with 40 components  </li> \n",
    "<li> **KPCA** : Associated with linear models. Outperformed by svm  </li>\n",
    "<li>**Linear Discriminant Analysis** : Outperformed by PCA  </li>\n",
    "<li> **Wavelets Transformation** : Outperformed by PCA + Edge detector  </li> \n",
    "<li>  **Augmented train set** : We tried to add some transformation invariance to our model by adding to the training dataset images obtained by images obtained by adding some transformations (translations and rotations) to the original images. However that appraoch was dropped because the improvement of performance we got was not worth it compare to the run time induced by doubling or tripling the learning dataset</li>\n",
    "</ul> \n",
    "\n",
    "\n",
    "### Models selection\n",
    "In that part we focused on optimising the 2 models (with kernels) implemented in scikit-learn that we thought were the most appropriated for a muti-class classification problem : **logistic regression** and **SVC**. <br/>\n",
    "A <u> kernelized SVC </u> appeared to be the best model with a 'rbf' kernel (sigma=0.02).\n",
    "\n",
    "### Partial conclusion\n",
    "At the end of that part we came up with a good framework to answer the given problem: <b> Binerization + Edge detector + PCA + SVC </b>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2 : Implementation\n",
    "The implementation of the edge detector was very straight forward (sliding windows)\n",
    "\n",
    "### PCA\n",
    "To implement the PCA, we used numpy to perform the eigen values decomposition of the covariance matrix of X. We simply troncate the first k eigenvalues after.\n",
    "\n",
    "### SVM\n",
    "#### Dual VS Primal\n",
    "We decided to propose 2 implementations of SVM :\n",
    "<ul> \n",
    "<li> One resolving the **primal** problem using *stochastic gradient descent* (theorical guarantees can be found in http://www.cs.huji.ac.il/~shais/papers/ShalevSiSrCo10.pdf)   </li> \n",
    "<li> One resolving the **dual** problem using a *quadratic solver* (CVXOPT) </li>\n",
    "</ul> \n",
    "\n",
    "We were happy to find that our SGD SVM was able to find good approximate solutions, for this problem, for reasonnable number of iterations. \n",
    "\n",
    "#### Multi-class\n",
    "In order to adapt to the multi-class case we implemented a **1 VS 1** approach. We lately compared it with a **1 VS all** approach and the later was better both in terms of time and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Compare to the other group, our score was not really good (94% accuracy) even though we used a very rigourous approach and spent a lot of time in pre processing.\n",
    "<br/>\n",
    "We tried to give explanations in the gap of perfomance between our models performances and those of our classmates.\n",
    "<ul> \n",
    "<li> Having computers that are not very poweful it took quite a lot of time to train our implemented algorithms (no parallelisation added). Therefore we didn't perform a model selection step and used the optimal parameters values obtained via the scikit-learn implementation (grid search ). There is propable that those parameters are not the best ones for our own made implementation  </li> \n",
    "<li> We had trouble when trying to use a gaussian kernel. We lately foud that the training features had to be normalized but we were incomfortable with the idea of normalizing features obtained after PCA. </li>\n",
    "<li> We also think that other pre processing could help achieve better performance, typically a better segmentation. </li>\n",
    "<li> We also thougth about replacing the hinge of the SVM primal formulation with others loss (sigmoid, least square) and use our SGD solver. Later use the 3 models to obtain better prediction by weigthed majority votes. However we did't have enough time to implement that idea. </li>\n",
    "\n",
    "</ul> \n",
    "Even if the digit recognition is a well know subject, implement an SVM without using library give us a better knowledge in how it work and how it can be addapted for different task. <br/>\n",
    "We are very curious to know what our classmates have done to obtain such great scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
