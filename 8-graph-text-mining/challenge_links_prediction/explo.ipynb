{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link prediction in citation graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MARIVAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MARIVAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from numpy import linalg as LA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import pylab\n",
    "import sklearn\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import math\n",
    "from tempfile import TemporaryFile\n",
    "import re\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes : train=(615512, 3), test=(32648, 2), nodes=(27770, 6)\n",
      "    source   target  label\n",
      "0  9510123  9502114      1\n",
      "1  9707075  9604178      1\n",
      "    source   target\n",
      "0  9807076  9807139\n",
      "1   109162     1182\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>2000</td>\n",
       "      <td>compactification geometry and duality</td>\n",
       "      <td>Paul S. Aspinwall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>these are notes based on lectures given at tas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>2000</td>\n",
       "      <td>domain walls and massive gauged supergravity p...</td>\n",
       "      <td>M. Cvetic, H. Lu, C.N. Pope</td>\n",
       "      <td>Class.Quant.Grav.</td>\n",
       "      <td>we point out that massive gauged supergravity ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>2000</td>\n",
       "      <td>comment on metric fluctuations in brane worlds</td>\n",
       "      <td>Y.S. Myung, Gungwon Kang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>recently ivanov and volovich hep-th 9912242 cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>2000</td>\n",
       "      <td>moving mirrors and thermodynamic paradoxes</td>\n",
       "      <td>Adam D. Helfer</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>quantum fields responding to moving mirrors ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>2000</td>\n",
       "      <td>bundles of chiral blocks and boundary conditio...</td>\n",
       "      <td>J. Fuchs, C. Schweigert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>proceedings of lie iii clausthal july 1999 var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1006</td>\n",
       "      <td>2000</td>\n",
       "      <td>questions in quantum physics</td>\n",
       "      <td>Rudolf Haag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>an assessment of the present status of the the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1007</td>\n",
       "      <td>2000</td>\n",
       "      <td>topological defects in 3-d euclidean gravity</td>\n",
       "      <td>Sheng Li, Yong Zhang, Zhongyuan Zhu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>by making use of the complete decomposition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1008</td>\n",
       "      <td>2000</td>\n",
       "      <td>n 0 supersymmetry and the non-relativistic mon...</td>\n",
       "      <td>Donald Spector</td>\n",
       "      <td>Phys.Lett.</td>\n",
       "      <td>we study some of the algebraic properties of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1009</td>\n",
       "      <td>2000</td>\n",
       "      <td>gluon pair production from space-time dependen...</td>\n",
       "      <td>Gouranga C. Nayak, Walter Greiner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we compute the probabilty for the processes a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1010</td>\n",
       "      <td>2000</td>\n",
       "      <td>instantons euclidean supersymmetry and wick ro...</td>\n",
       "      <td>A.V. Belitsky, S. V, oren, P. van Nieuwenhuizen</td>\n",
       "      <td>Phys.Lett.</td>\n",
       "      <td>we discuss the reality properties of the fermi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  year                                              title  \\\n",
       "0  1001  2000              compactification geometry and duality   \n",
       "1  1002  2000  domain walls and massive gauged supergravity p...   \n",
       "2  1003  2000     comment on metric fluctuations in brane worlds   \n",
       "3  1004  2000         moving mirrors and thermodynamic paradoxes   \n",
       "4  1005  2000  bundles of chiral blocks and boundary conditio...   \n",
       "5  1006  2000                       questions in quantum physics   \n",
       "6  1007  2000       topological defects in 3-d euclidean gravity   \n",
       "7  1008  2000  n 0 supersymmetry and the non-relativistic mon...   \n",
       "8  1009  2000  gluon pair production from space-time dependen...   \n",
       "9  1010  2000  instantons euclidean supersymmetry and wick ro...   \n",
       "\n",
       "                                           authors            journal  \\\n",
       "0                                Paul S. Aspinwall                NaN   \n",
       "1                      M. Cvetic, H. Lu, C.N. Pope  Class.Quant.Grav.   \n",
       "2                         Y.S. Myung, Gungwon Kang                NaN   \n",
       "3                                   Adam D. Helfer          Phys.Rev.   \n",
       "4                          J. Fuchs, C. Schweigert                NaN   \n",
       "5                                      Rudolf Haag                NaN   \n",
       "6              Sheng Li, Yong Zhang, Zhongyuan Zhu                NaN   \n",
       "7                                   Donald Spector         Phys.Lett.   \n",
       "8                Gouranga C. Nayak, Walter Greiner                NaN   \n",
       "9  A.V. Belitsky, S. V, oren, P. van Nieuwenhuizen         Phys.Lett.   \n",
       "\n",
       "                                            abstract  \n",
       "0  these are notes based on lectures given at tas...  \n",
       "1  we point out that massive gauged supergravity ...  \n",
       "2  recently ivanov and volovich hep-th 9912242 cl...  \n",
       "3  quantum fields responding to moving mirrors ha...  \n",
       "4  proceedings of lie iii clausthal july 1999 var...  \n",
       "5  an assessment of the present status of the the...  \n",
       "6  by making use of the complete decomposition of...  \n",
       "7  we study some of the algebraic properties of t...  \n",
       "8  we compute the probabilty for the processes a ...  \n",
       "9  we discuss the reality properties of the fermi...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF = pd.read_csv(\"training_set.txt\",header=None,sep=\" \")\n",
    "trainDF.columns=['source','target','label']\n",
    "testDF = pd.read_csv(\"testing_set.txt\",header=None,sep=\" \")\n",
    "testDF.columns=['source','target']\n",
    "node_infoDF = pd.read_csv(\"node_information.csv\",header=None)\n",
    "node_infoDF.columns=['ID','year','title','authors','journal','abstract']\n",
    "\n",
    "print(\"shapes : train={}, test={}, nodes={}\".format(trainDF.shape, testDF.shape, node_infoDF.shape))\n",
    "print (trainDF.head(2))\n",
    "print (testDF.head(2))\n",
    "node_infoDF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             0\n",
       "year           0\n",
       "title          0\n",
       "authors     4033\n",
       "journal     7472\n",
       "abstract       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_infoDF.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since very few features were usable in the raw data, we had to design new features from this raw data. We'll start by listing the features we designed, then we will try different classification algorithms with those features and tune the parameters of the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First set of features - Nodeinfo, graph structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract words tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sylvain: To compare two abstracts and measure how \"close\" they are to each other, we will use tfidf on the words in the abstract. We first make a list of the words in all the abstracts in order to compute the document frequency. This is the role of the dictionary dict0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique(a):\n",
    "    k=0\n",
    "    while k < len(a):\n",
    "        if a[k] in a[k+1:]:\n",
    "            a.pop(k)\n",
    "        else:\n",
    "            k=k+1\n",
    "            \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docfr will be used for the document frequency in the tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(dict0)):\n",
    "    b=0\n",
    "    for j in range(len(nodeinf)):\n",
    "        dict1=[]\n",
    "        dict1=dict1+re.split('\\s+', nodeinf[5][i])\n",
    "        if dict0[0][i] in dict1:\n",
    "            b=b+1\n",
    "    dict0[\"docfr\"][i]=b/len(nodeinf)\n",
    "\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sylvain:We will now remove the words with a small tfidf score (<1.7) since they are not very relevant. Then thanks to this dictionary we will be able to compute how many words with high tfidf score 2 documents have in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def applyfunc5(x):\n",
    "    x1=[]\n",
    "    x2=[]\n",
    "    for g2 in x:\n",
    "        if x[g2]>1.7:\n",
    "            x1=x1+[g2]\n",
    "            x2=x2+[x[g2]]\n",
    "    dico = dict(zip(x1,x2))\n",
    "    return(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodeinf[\"tfidf1\"]=0\n",
    "nodeinf[\"tfidf1\"]=nodeinf[\"tfidf\"].apply(applyfunc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "punct = string.punctuation.replace(\"-\", \"\")\n",
    "\n",
    "def tokenize(content, remove_stopwords=True, stemming=True):\n",
    "    # remove formatting\n",
    "    content =  re.sub(\"\\s+\", \" \", content)\n",
    "    # convert to lower case\n",
    "    content = content.lower()\n",
    "    # remove punctuation (preserving intra-word dashes)\n",
    "    content = \"\".join(letter for letter in content if letter not in punct)\n",
    "    # remove dashes attached to words but that are not intra-word\n",
    "    content = re.sub(\"[^[:alnum:]['-]\", \" \", content)\n",
    "    content = re.sub(\"[^[:alnum:][-']\", \" \", content)\n",
    "    # remove extra white space\n",
    "    content = re.sub(\" +\",\" \", content)\n",
    "    # remove leading and trailing white space\n",
    "    content = content.strip()\n",
    "    # tokenize\n",
    "    tokens = content.split(\" \")\n",
    "    # remove stopwords\n",
    "    if remove_stopwords==True:\n",
    "        tokens = [token for token in tokens if token not in stpwds]\n",
    "    if stemming==True:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of common authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the variable containing the number of common authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set2 = pd.DataFrame()\n",
    "training_set2[\"num_com_aut\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numcomau(x):\n",
    "    a1=x[\"source\"]\n",
    "    str1=nodeinf[nodeinf[0]==a1]\n",
    "    str1=str1.reset_index(drop=True)\n",
    "    str1=str1.loc[0,3]\n",
    "    if pd.isnull(str1)==False:\n",
    "        b1=re.split(',',str1)\n",
    "        cst=0\n",
    "        a2=x[\"target\"]\n",
    "        str2=nodeinf[nodeinf[0]==a2]\n",
    "        str2=str2.reset_index(drop=True)\n",
    "        str2=str2.loc[0,3]   \n",
    "        if pd.isnull(str2)==False:\n",
    "            b2=re.split(',', str2)\n",
    "            for j in range(len(b1)):\n",
    "                for k in range(len(b2)):\n",
    "                    if tokenize(b1[j])==tokenize(b2[k]):\n",
    "                        cst=cst+1\n",
    "            return(cst)\n",
    "        else:\n",
    "             return(0)\n",
    "    else:\n",
    "        return(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set2[\"num_com_aut\"]=training_set2.apply(numcomau,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time difference (difference between the dates of publication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the variable containing the time difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set2[\"tempdiff\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numcomau1(x):\n",
    "    a1=x[\"source\"]\n",
    "    str1=nodeinf[nodeinf[0]==a1]\n",
    "    str1=str1.reset_index(drop=True)\n",
    "    str1=int(str1.loc[0,1])\n",
    "    if pd.isnull(str1)==False:\n",
    "        a2=x[\"target\"]\n",
    "        str2=nodeinf[nodeinf[0]==a2]\n",
    "        str2=str2.reset_index(drop=True)\n",
    "        str2=int(str2.loc[0,1])   \n",
    "        if pd.isnull(str2)==False:\n",
    "            return(str1-str2)\n",
    "        else:\n",
    "            return(1)\n",
    "    else:\n",
    "        return(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set2[\"tempdiff\"]=training_set2.apply(numcomau1,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of common words in the title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a varibale containing the number of common words in two documents' titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set2[\"numcomwordintitle\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numcomau2(x):\n",
    "    a1=x[\"source\"]\n",
    "    str1=nodeinf[nodeinf[0]==a1]\n",
    "    str1=str1.reset_index(drop=True)\n",
    "    str1=str1.loc[0,2]\n",
    "    if pd.isnull(str1)==False:\n",
    "        b1=re.split(' ',str1)\n",
    "        cst=0\n",
    "        a2=x[\"target\"]\n",
    "        str2=nodeinf[nodeinf[0]==a2]\n",
    "        str2=str2.reset_index(drop=True)\n",
    "        str2=str2.loc[0,2]   \n",
    "        if pd.isnull(str2)==False:\n",
    "            b2=re.split(' ',str2)\n",
    "            for j in range(len(b1)):\n",
    "                for k in range(len(b2)):\n",
    "                    if b1[j]==b2[k]:\n",
    "                        cst=cst+1\n",
    "            return(cst)\n",
    "        else:\n",
    "            return(0)\n",
    "    else:\n",
    "         return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set2[\"numcomwordintitle\"]=training_set2.apply(numcomau2,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Were the articles published in the same journal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a categorical variable equal to 1 if two artcles were published in the same article, 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set2[\"samejournal\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numcomau3(x):\n",
    "    a1=x[\"source\"]\n",
    "    str1=nodeinf[nodeinf[0]==a1]\n",
    "    str1=str1.reset_index(drop=True)\n",
    "    str1=str1.loc[0,4]\n",
    "    if pd.isnull(str1)==False:\n",
    "        \n",
    "        cst=0\n",
    "        a2=x[\"target\"]\n",
    "        str2=nodeinf[nodeinf[0]==a2]\n",
    "        str2=str2.reset_index(drop=True)\n",
    "        str2=str2.loc[0,4]   \n",
    "        if pd.isnull(str2)==False:\n",
    "\n",
    "            if str1==str2:\n",
    "                cst=1\n",
    "            return(cst)\n",
    "        else:\n",
    "            return(0)\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set2[\"samejournal\"]=training_set2.apply(numcomau3,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of common words in the abstract with a high tfidf score (>1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set2[\"numofightfidfwordsincom\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numcomau4(x):\n",
    "    a1=x[\"source\"]\n",
    "    str1=nodeinf[nodeinf[0]==a1]\n",
    "    str1=str1.reset_index(drop=True)\n",
    "    str1=str1.loc[0,\"tfidf1\"]\n",
    "    if pd.isnull(str1)==False:\n",
    "        \n",
    "        cst=0\n",
    "        a2=x[\"target\"]\n",
    "        str2=nodeinf[nodeinf[0]==a2]\n",
    "        str2=str2.reset_index(drop=True)\n",
    "        str2=str2.loc[0,\"tfidf1\"]\n",
    "        if pd.isnull(str2)==False:\n",
    "\n",
    "            for j in str1:\n",
    "                for k in str2:\n",
    "                    if j==k:\n",
    "                        cst=cst+1\n",
    "            return(cst)\n",
    "        else:\n",
    "             return(0)\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set2[\"numofightfidfwordsincom\"]=training_set2.apply(numcomau4,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of times the author of the source paper cited a paper published in the same journal as the target paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set2[\"nbdefoisautcitejournarttarget\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(training_set2)):\n",
    "    a1=training_set2[\"source\"][i]\n",
    "    str1=nodeinf[nodeinf[0]==a1]\n",
    "    str1=str1.reset_index(drop=True)\n",
    "    str1=str1.loc[0,3]\n",
    "    if pd.isnull(str1)==False:\n",
    "        str1=removepar(str1)\n",
    "        b1=re.split(',',str1)\n",
    "        a2=training_set2[\"target\"][i]\n",
    "        str2=nodeinf[nodeinf[0]==a2]\n",
    "        str2=str2.reset_index(drop=True)\n",
    "        str2=str2.loc[0,4]   \n",
    "        if pd.isnull(str2)==False:\n",
    "            cst=0\n",
    "            j1=0\n",
    "            for k in b1:\n",
    "                if len(k)>3:\n",
    "                    if k in list(dictaut[0]):\n",
    "                        str3=dictaut[dictaut[0]==k].reset_index(drop=True).loc[0,\"papers\"]\n",
    "                        if str2 in str3:\n",
    "                            cst=cst+str3[str2]\n",
    "                    else:\n",
    "\n",
    "                        for j in range(len(dictaut)):\n",
    "                            if k in dictaut[0][j]:\n",
    "                                str3=dictaut .loc[j,\"papers\"]\n",
    "                                if str2 in str3:\n",
    "                                    cst=cst+str3[str2]\n",
    "                                j1=1+j1\n",
    "                    \n",
    "            cst=cst/(len(b1)+j1)\n",
    "            training_set2[\"nbdefoisautcitejournarttarget\"][i]=cst\n",
    "        else:\n",
    "             training_set2[\"nbdefoisautcitejournarttarget\"][i]=0\n",
    "    else:\n",
    "        training_set2[\"nbdefoisautcitejournarttarget\"][i]=0\n",
    "    if i%100==0:\n",
    "        print(i/len(training_set2)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth-first\tsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sylvain :Shortest distance between two nodes. The code is quite long and boring so we didn't put it here. You can however find it in the annexe (ALTEGRAG.ipynb). We count how many edges we have to go through in order to go from a node to another. In our case some nodes the distance is only 1 as one paper can cite another. In order for the algorithm to predict we should not take into account direct connection between nodes. Indeed we dont have this information on the testing set. Therefore we have adapted the algorithm so that it doesn't take into account 2 adjacent nodes. We computed this algorithm by multiplying the adjacency matrix. As this matrix is very large, we used dictionaries to compute the multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature X - Pagerank and HITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will attempt to pagerank the papers (nodes of the citation graph) to create a feature that we will help us predict missing citation links between papers. We will also compute the authors graph (there is a link from A to B if A has cited B). The authors pagerank and their hits score (hubness and authority) will also be used as new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper pagerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create the paper/citation graph with the training data trainDF. We used Networkx to create the graph: http://networkx.github.io/documentation/latest/_downloads/networkx_reference.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GraphData = nx.DiGraph()\n",
    "for i in range(int(trainDF.shape[0])):\n",
    "    source_ID = trainDF.source[i]\n",
    "    target_ID = trainDF.target[i]\n",
    "    label = trainDF.label[i]\n",
    "    if (label==1):\n",
    "        GraphData.add_edge(source_ID,target_ID) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x24f0bc73e10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GraphData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid too much computing, we saved useful variables in a pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('pagerankVariables.p', 'wb')\n",
    "pickle.dump(GraphData, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute the pagerank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr = nx.pagerank(GraphData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>pagerank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9306112</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9306114</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9306115</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9306116</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9306117</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source  pagerank\n",
       "0  9306112  0.000011\n",
       "1  9306114  0.000011\n",
       "2  9306115  0.000023\n",
       "3  9306116  0.000018\n",
       "4  9306117  0.000065"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converts dictionary to dataframe \n",
    "prDF = pd.DataFrame(list(pr.items()), columns = ['source','pagerank'])\n",
    "prDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('pagerankVariables.p', 'wb')\n",
    "pickle.dump(prDF, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create the author graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cited_auths(id1,id2):\n",
    "    source_auth = node_infoDF.ix[node_infoDF.ID==id1,\"authors\"]\n",
    "    target_auth = node_infoDF.ix[node_infoDF.ID==id2,\"authors\"]\n",
    "    if source_auth.isnull().values or target_auth.isnull().values: #a simplifier\n",
    "        return 0\n",
    "    else:\n",
    "        return (set(source_auth.values[0].split(\",\")),set(target_auth.values[0].split(\",\")))\n",
    "cit_test = cited_auths(1001,1002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Paul S. Aspinwall'}, {' C.N. Pope', ' H. Lu', 'M. Cvetic'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cit_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes the IDs of two papers and returns a couple where the first element is the set of authors who wrote the source article and the second element the set of authors who wrote the target article. To create the authors graph we will create a link between every pair (source_author,target_author)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GraphAuths = nx.DiGraph()\n",
    "for i in range(int(trainDF.shape[0])):\n",
    "    source_ID = trainDF.source[i]\n",
    "    target_ID = trainDF.target[i]\n",
    "    label = trainDF.label[i]\n",
    "    if (label == 1):\n",
    "        cited_list = cited_auths(source_ID, target_ID)\n",
    "        if (cited_list == 0):\n",
    "            pass\n",
    "        else:\n",
    "            for source_author in list(cited_list[0]):\n",
    "                for target_author in list(cited_list[1]):\n",
    "                    GraphAuths.add_edge(source_author,target_author)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('pagerankVariables.p', 'wb')\n",
    "pickle.dump(GraphAuths, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the pagerank :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth_pr = nx.pagerank(GraphAuths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_pr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brett J. Taylor</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A. Sugamoto</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mariano J. Salvay</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M.N. Stoilov</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T. Kojima</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author  author_pr\n",
       "0     Brett J. Taylor   0.000011\n",
       "1         A. Sugamoto   0.000015\n",
       "2   Mariano J. Salvay   0.000010\n",
       "3        M.N. Stoilov   0.000010\n",
       "4           T. Kojima   0.000070"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converts dictionary to dataframe \n",
    "auth_prDF = pd.DataFrame(list(auth_pr.items()), columns = ['author','author_pr'])\n",
    "auth_prDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('pagerankVariables.p', 'wb')\n",
    "pickle.dump(auth_prDF, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the HITS scores :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth_hits = nx.hits(GraphAuths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nx.hits(GraphAuths) returns a couple containing two dictionnaries, the first one (auth_hits[0]) with the hubnesses of each node, the second one (auth_hits[1]) with the authorities of the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_hubs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brett J. Taylor</td>\n",
       "      <td>9.540999e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A. Sugamoto</td>\n",
       "      <td>6.462332e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mariano J. Salvay</td>\n",
       "      <td>8.582825e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J. Maldacena</td>\n",
       "      <td>8.747089e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T. Kojima</td>\n",
       "      <td>8.430559e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author   author_hubs\n",
       "0     Brett J. Taylor  9.540999e-05\n",
       "1         A. Sugamoto  6.462332e-05\n",
       "2   Mariano J. Salvay  8.582825e-07\n",
       "3        J. Maldacena  8.747089e-04\n",
       "4           T. Kojima  8.430559e-06"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converts dictionary to dataframe \n",
    "auth_hubsDF = pd.DataFrame(list(auth_hits[0].items()), columns = ['author','author_hubs'])\n",
    "auth_hubsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('pagerankVariables.p', 'wb')\n",
    "pickle.dump(auth_hubsDF, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_authorities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brett J. Taylor</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A. Sugamoto</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mariano J. Salvay</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J. Maldacena</td>\n",
       "      <td>0.001670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T. Kojima</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>K.Selivanov</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P. Brax</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M.Tarlini</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yasuyuki Sekiguchi</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hiroshi Ishikawa (Tohoku Univ.)</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            author  author_authorities\n",
       "0                  Brett J. Taylor            0.000002\n",
       "1                      A. Sugamoto            0.000028\n",
       "2                Mariano J. Salvay            0.000000\n",
       "3                     J. Maldacena            0.001670\n",
       "4                        T. Kojima            0.000007\n",
       "5                      K.Selivanov            0.000003\n",
       "6                          P. Brax            0.000031\n",
       "7                        M.Tarlini            0.000019\n",
       "8               Yasuyuki Sekiguchi            0.000012\n",
       "9  Hiroshi Ishikawa (Tohoku Univ.)            0.000017"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converts dictionary to dataframe \n",
    "auth_authoritiesDF = pd.DataFrame(list(auth_hits[1].items()), columns = ['author','author_authorities'])\n",
    "auth_authoritiesDF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('pagerankVariables.p', 'wb')\n",
    "pickle.dump(auth_authoritiesDF, f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
