{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sys\n",
    "sys.path.append('DeepLearningMovies/')\n",
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "txt1 = \"i characters characters really fucking love that movie. I enjoyed the charaters play and so on. looking forward a movie like that .... \"\n",
    "txt2 = \"It was so annoying. i hate that type of movie :(\"\n",
    "rootdirPOS =\"./data/train/pos\"\n",
    "rootdirNEG =\"./data/train/neg\"\n",
    "pos = os.listdir(\"./data/train/pos\")\n",
    "neg = os.listdir(\"./data/train/neg\")\n",
    "train = []\n",
    "for i in range(100):\n",
    "    with open(\"./data/train/pos/\"+pos[i], 'r') as content_file:\n",
    "        content = content_file.read();\n",
    "        train.append(content)\n",
    "    with open(\"./data/train/neg/\"+neg[i], 'r') as content_file:\n",
    "        content = content_file.read();\n",
    "        train.append(content)\n",
    "y_train=np.ones(len(train))\n",
    "y_train[y_train.shape[0]/2:]=0\n",
    "\n",
    "test = []\n",
    "fil = os.listdir(\"./data/test/\")\n",
    "for i in range(200):\n",
    "    with open(\"./data/test/\"+fil[i], 'r') as content_file:\n",
    "        content = content_file.read();\n",
    "        test.append(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "nothing to repeat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-95c2744bb625>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mclean_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc2words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[0mclean_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc2words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-95c2744bb625>\u001b[0m in \u001b[0;36mdoc2words\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# text preprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"*http*\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[^0-9a-zA-Z]\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/zapfack/tools/anaconda2/lib/python2.7/re.pyc\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 155\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/zapfack/tools/anaconda2/lib/python2.7/re.pyc\u001b[0m in \u001b[0;36m_compile\u001b[1;34m(*key)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;31m# invalid expression\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbypass_cache\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: nothing to repeat"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def doc2words(doc):\n",
    "\t# stop-words based on the previous lab we add nltk stopwords\n",
    "\tstop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "\t\t\t\t'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "                'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "                'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "                'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "                'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "                'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "                'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "                'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "                'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "                'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "                'than', 'too', 'very', 's', 't', 'can','could', 'will','would', 'just', 'don', 'should', 'now','films',\n",
    "                'film','movies','movie','br','http','also','seem', ' ', '']\n",
    "\tstop_words.extend([w.encode('ascii','ignore') for w in stopwords.words(\"english\")])\n",
    "\t\n",
    "\t# text preprocessing \n",
    "\tdoc = re.sub(\"[^0-9a-zA-Z]\",\" \", doc)\n",
    "\tfor w in [\"'s\",\"'t\",\"'m\",\"'re\"]:\n",
    "\t\tdoc=doc.replace(w,' ')\n",
    "\t\n",
    "\twords=doc.strip().split(' ')\n",
    "\twords = [w.lower() for w in words]\n",
    "\tbigrams = zip(words,words[1:])\n",
    "\ttrigrams = zip(words,words[1:],words[2:])\n",
    "\t# Remove punctuation and lowercase\n",
    "\tpunctuation = set(string.punctuation)\n",
    "\tstop_words.extend(punctuation) \n",
    "\tstop_words.extend([w for w in words if len(w)<3]) \n",
    "\tstop_words = list(set(stop_words))  \n",
    "\t\n",
    "\tbigrams = [\" \".join(gram) for gram in bigrams if not any(i in stop_words for i in gram)]\n",
    "\ttrigrams = [\" \".join(gram) for gram in trigrams if not any(i in stop_words for i in gram)]\n",
    "\twords = [w for w in words if w not in stop_words and w not in punctuation]\n",
    "\twords.extend(bigrams)\n",
    "\twords.extend(trigrams)\n",
    "\n",
    "\t# # Stemming\n",
    "\t#stemmer = PorterStemmer()\n",
    "\t#words = [stemmer.stem(unicode(w, \"utf-8\")) for w in words] \n",
    "\t#print len(words)\n",
    "\treturn words\n",
    "\n",
    "clean_train = [\" \".join(doc2words(doc)) for doc in train]\n",
    "clean_test = [\" \".join(doc2words(doc)) for doc in test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None)\n",
    "train_feautures = vectorizer.fit_transform(clean_train)\n",
    "train_feautures = train_feautures.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'felix unger jack lemmon dumped wife one annoying neurotic people world suicide way get work heads friends house oscar madison walther matthau recently divorced living bachelor heaven smoking gambling hitting chicks eating never cleaning paradise well suicide attempts oscar decides let felix move first match made heaven felix cooks cleans helps oscar pay alimony time soon oscar jonesing women felix today world probably gay isn ready move invite couple british birds find felix tender soon felix weeping chatting family life leaving oscar denied explodes throws felix isn helpless seems soon upper hand favorite quote leave little notes pillow told 158 times stand little notes pillow cornflakes took three hours figure felix ungar based neil simon play wrote screenplay certain theatre feel set repartee looks feel quite play like better worse lemmon mathau excellent comedic chemistry appeared grumpier old men sea reprising finicky slob roles different names avoid royalty issues sure like strawberries dipped chocolate chocolate smooth sweet rich strawberry tart juicy bright red unless get nasty greenish ones almost polar opposites together contrasts highlight make wonderful dessert blog myspace com locoformovies felix unger jack lemmon neurotic people friends house oscar madison walther matthau recently divorced bachelor heaven never cleaning suicide attempts attempts oscar oscar decides let felix felix move match made felix cooks helps oscar oscar pay soon oscar british birds find felix family life leaving oscar oscar denied felix isn upper hand favorite quote little notes 158 times stand little little notes three hours felix ungar neil simon simon play certain theatre theatre feel looks feel feel quite quite play play like excellent comedic comedic chemistry grumpier old old men finicky slob slob roles different names avoid royalty royalty issues like strawberries strawberries dipped bright red nasty greenish greenish ones almost polar polar opposites contrasts highlight wonderful dessert blog myspace myspace com com locoformovies suicide attempts oscar attempts oscar decides let felix move helps oscar pay leaving oscar denied stand little notes neil simon play certain theatre feel looks feel quite feel quite play quite play like excellent comedic chemistry grumpier old men finicky slob roles avoid royalty issues like strawberries dipped nasty greenish ones almost polar opposites blog myspace com myspace com locoformovies'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f77bd26032c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'labeledTrainData.tsv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m                     \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'testData.tsv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m,\u001b[0m                    \u001b[0mquoting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'The first review is:'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train = pd.read_csv(os.path.join(os.path.dirname(__file__), 'data', 'labeledTrainData.tsv'), header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "    test = pd.read_csv(os.path.join(os.path.dirname(__file__), 'data', 'testData.tsv'), header=0, delimiter=\"\\t\", \\\n",
    "                   quoting=3 )\n",
    "\n",
    "    print 'The first review is:'\n",
    "    print train[\"review\"][0]\n",
    "\n",
    "    raw_input(\"Press Enter to continue...\")\n",
    "\n",
    "\n",
    "    print 'Download text data sets. If you already have NLTK datasets downloaded, just close the Python download window...'\n",
    "    #nltk.download()  # Download text data sets, including stop words\n",
    "\n",
    "    # Initialize an empty list to hold the clean reviews\n",
    "    clean_train_reviews = []\n",
    "\n",
    "    # Loop over each review; create an index i that goes from 0 to the length\n",
    "    # of the movie review list\n",
    "\n",
    "    print \"Cleaning and parsing the training set movie reviews...\\n\"\n",
    "    for i in xrange( 0, len(train[\"review\"])):\n",
    "        clean_train_reviews.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(train[\"review\"][i], True)))\n",
    "\n",
    "\n",
    "    # ****** Create a bag of words from the training set\n",
    "    #\n",
    "    print \"Creating the bag of words...\\n\"\n",
    "\n",
    "\n",
    "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "    # bag of words tool.\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000)\n",
    "\n",
    "    # fit_transform() does two functions: First, it fits the model\n",
    "    # and learns the vocabulary; second, it transforms our training data\n",
    "    # into feature vectors. The input to fit_transform should be a list of\n",
    "    # strings.\n",
    "    train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "    # Numpy arrays are easy to work with, so convert the result to an\n",
    "    # array\n",
    "    train_data_features = train_data_features.toarray()\n",
    "\n",
    "    # ******* Train a random forest using the bag of words\n",
    "    #\n",
    "    print \"Training the random forest (this may take a while)...\"\n",
    "\n",
    "\n",
    "    # Initialize a Random Forest classifier with 100 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as\n",
    "    # features and the sentiment labels as the response variable\n",
    "    #\n",
    "    # This may take a few minutes to run\n",
    "    forest = forest.fit( train_data_features, train[\"sentiment\"] )\n",
    "\n",
    "\n",
    "\n",
    "    # Create an empty list and append the clean reviews one by one\n",
    "    clean_test_reviews = []\n",
    "\n",
    "    print \"Cleaning and parsing the test set movie reviews...\\n\"\n",
    "    for i in xrange(0,len(test[\"review\"])):\n",
    "        clean_test_reviews.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(test[\"review\"][i], True)))\n",
    "\n",
    "    # Get a bag of words for the test set, and convert to a numpy array\n",
    "    test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "    test_data_features = test_data_features.toarray()\n",
    "\n",
    "    # Use the random forest to make sentiment label predictions\n",
    "    print \"Predicting test labels...\\n\"\n",
    "    result = forest.predict(test_data_features)\n",
    "\n",
    "    # Copy the results to a pandas dataframe with an \"id\" column and\n",
    "    # a \"sentiment\" column\n",
    "    output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "    # Use pandas to write the comma-separated output file\n",
    "    output.to_csv(os.path.join(os.path.dirname(__file__), 'data', 'Bag_of_Words_model.csv'), index=False, quoting=3)\n",
    "    print \"Wrote results to Bag_of_Words_model.csv\"\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
