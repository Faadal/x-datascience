{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIG DATA ANALYTICS  PROJECT  “Opinion Mining  with Spark”  REPORT\n",
    "**Fabrice ZAPFACK, Sofia HAFDANI, Svetlana Smagina**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *!!! Don't run this codes as they are intended to be run in a pyspark context* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In this project we faced a classification problem, the being to predict if a particular text correspond to a **good** or a **bad** review.The problem therefore correspond to a sentiment analysis problem.*\n",
    "\n",
    "To resolve this problem, we are given 2 datasets :\n",
    "1.  A set of 25,000 documents that contain labeled reviews either as positive or negative (50%-50%). This will be used for TRAINING. \n",
    "2. Another set of 25.000 documents containing unlabeled reviews that we need to assign labels to them. This set will be used for TESTING. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First approach : Resilient Distributed Datasets (RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step in this project, we decided to implement our models using spark in order to parallelize the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data files were availables for the course website. A python script, **< loadFiles.py>** was given to be able to import those files.\n",
    "* To load the training data, the path of the directory needs to be passed as argument and it returns a python list containing the 25000 text reviews and a target vector (numpy) containing the label of each text (1 if positive an 0 if negative)\n",
    "* To load the test data, the path of the directory needs to be passed as argument and it returns a python list containing the 25000 text reviews and a list containing the name each text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import loadFiles as lf\n",
    "data,Y=lf.loadLabeled(\"./data/train\")\n",
    "test,names=lf.loadUknown('./data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "### Punctuation removal\n",
    "The first step of the preprocessing consist of removing the punctuations (to help plitting the documument in words). For that we first used for loop to replace punctuations by \" \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for w in ['.',',','--',':','!','?','(',')','\"','/','<','>']:\n",
    "\tdoc=doc.replace(w,' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used instead regular expression from the package **< re >**. Depending of the features we wanted to extract, we conserved either alpha-numeric characters (bag of words appraoches) or alphabetical (word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "doc = re.sub(\"[^a-zA-Z]\",\" \", doc) #alphabetical characters conserved\n",
    "doc = re.sub(\"[^0-9a-zA-Z]\",\" \", doc) #alpha-numeric characters conserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to remove all the punctuations even we know that somitimes they are linked to sentiments (\"!!!\", \":-)\", \":(\", ...). Also some html tags where presents in the texts. It is recommanded to remove them using powerful packages like **<beautifulSoup>** but we considered it was not worth to do it as that packages were not present in the machines cluster and removed those tag simply by adding words like **'html', 'br', ...** in the stopwords list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document spliting and lowercase\n",
    "The documents were then splitted in a list of words and each of those words were transformed to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words=doc.strip().split(' ')\n",
    "words = [w.lower() for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords removal\n",
    "The stopwords used in this project were a concatenation of self-made stopwords and the ones given in the nltk package. We also decided to remove words which contains less than 3 characters.\n",
    "** the stopwords where removed only for the bag-of-words approaches **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stop-words based on the previous lab we add nltk stopwords\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "\t\t\t'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "                'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "                'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "                'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "                'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "                'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "                'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "                'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "                'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "                'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "                'than', 'too', 'very', 's', 't', 'can','could', 'will','would', 'just', 'don', 'should', 'now','films',\n",
    "                'film','movies','movie','br','http','also','seem', ' ', '']\n",
    "stop_words.extend([w.encode('ascii','ignore') for w in stopwords.words(\"english\")])\n",
    "stop_words.extend([w for w in words if len(w)<3]) \n",
    "stop_words = list(set(stop_words)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming \n",
    "We also used porter stemmenr, implemented in nltk for stemming. However stemming was removed in our final submission as it decreases the performance obtained by cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "words = [stemmer.stem(unicode(w, \"utf-8\")) for w in words] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams\n",
    "We decided to limit our word with 3-grams because believe that bigger features will capture more noise than information and also because the number of features was already very big when using 3-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams = zip(words,words[1:])\n",
    "trigrams = zip(words,words[1:],words[2:])\n",
    "bigrams = [\" \".join(gram) for gram in bigrams if not any(i in stop_words for i in gram)]\n",
    "trigrams = [\" \".join(gram) for gram in trigrams if not any(i in stop_words for i in gram)]\n",
    "words = [w for w in words if w not in stop_words and w not in punctuation]\n",
    "words.extend(bigrams)\n",
    "words.extend(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction\n",
    "\n",
    "#### Latent Dirichlet allocation\n",
    "We fitted an LDA to reduce dimensionality to the most relevant topics: \n",
    "Latent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Extraction\n",
    "\n",
    "### Bag of words approach\n",
    "\n",
    "#### Presence/absence of word \n",
    "This is a simple approach where a document is represented by a vector (sparse representation) where each column is 0 or 1 (1 if the corresponding word appears in the document 0 if not). We used the code provided in the function **< createBinaryLabeledPoint >**.\n",
    "\n",
    "#### TF-IDF \n",
    "This approach is similar to previous one. There differnce is that instead of representing a document by 0/1 vector, it is now represented by vector containing the tf-idf of each words in the document.\n",
    " - First we compute the dictionnary \n",
    " - For each document \n",
    "     - we split the document in a list of words (tokenization)\n",
    "     - we count the number of occurrence of each words using **< HashingTF >** (TF)\n",
    "     - we multiply the tf by the idf (inverse proportionnal of the occurence of the word in the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.feature import IDF\n",
    "dataRDD=sc.parallelize(data,numSlices=num_slices)\n",
    "lists=dataRDD.map(doc2words)\n",
    "# create dict\n",
    "all=[]\n",
    "for l in lists.collect():\n",
    "\tall.extend(l)\n",
    "dict=set(all)\n",
    "\n",
    "# TF-IDF\n",
    "print \"len dict {} \".format(len(dict)) \n",
    "hashingTF = HashingTF(numFeatures=len(dict))\n",
    "tf = hashingTF.transform(lists)\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Feature Engineering with: Word2Vec method\n",
    "\n",
    "We first need to perform tokenization word2vec is suppose to perform better without stopword removal.\n",
    "We then use the lists of words to train a word2vec model implemented in < mllib>.\n",
    "As reference, we learned that: \n",
    "Word2vec is a group of related models that are used to produce so-called word embeddings. These models are shallow, two-layer neural networks, that are trained to reconstruct linguistic contexts of words: the network is shown a word, and must guess at which words occurred in adjacent positions in an input text. The order of the remaining words is not important (bag-of-words assumption) (Mikolov, Tomas; et.al (2013). \"Distributed representations of words and phrases and their compositionality\") \n",
    "After training, word2vec models can be used to map each word to a vector of typically several hundred elements, which represent that word's relation to other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "dataRDD=sc.parallelize(data,numSlices=num_slices)\n",
    "sentencesRDD = dataRDD.map(lambda x: doc2words(unicode(x,errors='ignore')))\n",
    "print \"Start word2vec\"\n",
    "start = time.time()\n",
    "word2vec = Word2Vec().setVectorSize(200).setSeed(42).setMinCount(50)\n",
    "model = word2vec.fit(sentencesRDD)\n",
    "words_vect=model.getVectors()\n",
    "end = time.time()\n",
    "print \"End word2vec learning. duration {} s\".format(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments on the word2vec feature engineering method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It seems that this model does not capture well the sentiment in the text as the synonyms found are not very relevant. We therefore tried a second time using the words 'comment', 'comments', 'user', 'users', 'imdb', 'movie', 'movies', 'film', 'films' as stopwords but without success.\n",
    "\n",
    "The implementation of word2vec used doesn't directly give the matrix containing the vector representation of the words. To compute it, we first computed the dictionnary of the corpus (excluding the stopwords), and each term in the dictionary was then transformed in the word vector space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lists=dataRDD.map(doc2words).collect()\n",
    "all=[]\n",
    "for l in lists:\n",
    "\tall.extend(l)\n",
    "dictionary=set(all)\n",
    "print \"Number of elements in dictionnary %d\" % (len(dictionary))\n",
    "words_vectors = {}\n",
    "test = {}\n",
    "for x in dictionary:\n",
    "\tprint x\n",
    "\ttest[x] = x\n",
    "\twords_vectors[x] = model.transform(x)\n",
    "end = time.time()\n",
    "print \"time {}\".format(end-start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "In this project we tested the 3 classification models implemented in < mllib > :\n",
    "- Naives Bayes\n",
    "- Linear SVM\n",
    "- Logistic regression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics\n",
    "To evaluate the performance of thes algorithms, we decide to use ** accuracy** as criteria. We have decided to use only this parameter because it is the only one that is used for evaluation by the examinator. We could have used other ones as precision, recall, auc, f2-score, confusion matrix ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "The models where evualuted using cross-validation. To do that we performed a shuffle split on the labelled training data because the initial data set was ordored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Sample of the code \n",
    "from  pyspark.mllib.classification import NaiveBayes\n",
    "from  pyspark.mllib.classification import SVMWithSGD\n",
    "from  pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "### cross-validaton\n",
    "cv = ShuffleSplit(len(tfidf), n_iter=3, test_size=0.3, random_state=42)\n",
    "models = [NaiveBayes, LogisticRegressionWithLBFGS, SVMWithSGD]\n",
    "scores = {model.__name__: [] for model in models}\n",
    "for i_cv, i_temp in enumerate(cv):\n",
    "    i_train = i_temp[0]\n",
    "\ti_test = i_temp[1]\n",
    "\tdata_train = [data[i] for i in i_train]\n",
    "\tY_train = [Y[i] for i in i_train]\n",
    "\tdata_test = [data[i] for i in i_test]\n",
    "\tY_test = [Y[i] for i in i_test]\n",
    "    ...\n",
    "    ...\n",
    "    for model in models:\n",
    "### Model training\n",
    "\t\tmodel_trained=model.train(labeledRDD)\n",
    "\t\tmb=sc.broadcast(model_trained)\n",
    "        \n",
    "### Model testing\n",
    "\t\tY_pred = model_trained.predict(testRDD2).collect()\n",
    "\t\tscore = accuracy_score(Y_test, Y_pred)\n",
    "\t\tscores[model.__name__].append(score)\n",
    "for key, value in scores.items():\n",
    "\tprint \"%s : mean=%.5f, std=%.5f \" %(key, np.mean(value), np.std(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters tuning\n",
    "We perfomed gridsearch in order to optimize hyperparameters, the loop we implemented returns scores that allow us to choose the best hyperparameters for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Gridsearch Implemented on spark\n",
    "\n",
    "grids = [ [{'lambda_':1.0 },{'lambda_':10.0 }], \n",
    "[{\"iterations\":100, \"initialWeights\":None, \"regParam\":0.01, 'regType':'l2', 'intercept':False, 'corrections':10, 'tolerance':0.0001, 'validateData':True, 'numClasses':2}],\n",
    "[{'iterations':100, 'step':1.0, 'regParam':0.01, 'miniBatchFraction':1.0, 'initialWeights':None, 'regType':'l2', 'intercept':False, 'validateData':True}] \n",
    "]\n",
    "### Each model has its own parameters: \n",
    "\n",
    "-SVM => terations=100, step=1.0, regParam=0.01, miniBatchFraction=1.0, initialWeights=None, regType='l2', intercept=False, validateData=True\n",
    "\n",
    "-LR => terations=100, initialWeights=None, regParam=0.01, regType='l2', intercept=False, corrections=10, tolerance=0.0001, validateData=True, numClasses=2\n",
    "\n",
    "-NaiveBayes => lambda_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second approach : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Random forests\n",
    "\n",
    "-Extra trees classifier\n",
    "\n",
    "-SGDClassifier with l1, l2, and elastic net penalizations\n",
    "\n",
    "-DecisionTreeClassifier\n",
    "\n",
    "-Ada Boost classifier\n",
    "\n",
    "-Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization of non-distributed models\n",
    "We also wanted to fit more advanced models, however, due to the lack of resources on Spark's machine learning libraries we decided to implement them using scikit learn. \n",
    "The code describing more thouroughtly this approach is available in our ipython notebook 'second.ipynb '. \n",
    "We wanted to reduce the dimensionality of the data using an LDA implemented on scikit learn. The code ran smoothly locally but did not work on the cluster because it required an updated version of scikit learn (0.17). For this reason, we tried to use the lda package \"lda 1.0.3\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection:\n",
    "\n",
    "In this second approach, feature selection was partly induced by the parameter tuning of the 'Countvectorizer' function from scikit learn:\n",
    "-Max_df parameter: allows to ignore terms that occured in too many documents\n",
    "-Min_df parameter: allows to ignore terms that occured in too few documents\n",
    "-Max_features parameter: allows to perform more feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Sample of our code \n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.grid_search import RandomizedSearchCV \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.ensemble import ExtraTreesClassifier\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from scipy.stats import randint as sp_randint\n",
    "from numpy.random import rand\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_iter_search = 3\n",
    "param_dist = {\"rf__n_estimators\":sp_randint(1,100),\n",
    "              \"rf__max_features\":sp_randint(1,20),\n",
    "               \"rf__max_depth\" :sp_randint(1,20),\n",
    "              \"rf__max_features\":sp_randint(1,20)\n",
    "            }\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = RandomForestClassifier()\n",
    "clf3 = GaussianNB()\n",
    "class Classifier(BaseEstimator):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pipeline= Pipeline([\n",
    "              \n",
    "             ('rf',RandomForestClassifier(n_jobs=-1) )    \n",
    "        ])\n",
    "        \n",
    "        #('vc', VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('nb', clf3)], voting='soft',weights=))\n",
    "        self.clf = RandomizedSearchCV(pipeline, param_distributions= param_dist, n_iter=n_iter_search)    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.clf.fit(X, y)\n",
    "        report(self.clf.grid_scores_)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.clf.predict_proba(X)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "1. RandomForestClassifier--> we tuned our parameters with a randomized grid search, obtained the best hyperparameters but the accuracy did not exceed 0.81\n",
    "\n",
    "2. We also tried other models such as ExtraTreesClassifier and DecisionTreeClassifier, however even with a thouroughtly tuning the accuracy did not exceed 0.79\n",
    "3. Finally we performed a voting classifier. This method is one of the ensemble methods that performs soft voting and majority voting for many estimates. We chose the ‘soft’ voting method which, predicts the class label based on the argmax of the sums of the predicted probalities, which is recommended for an ensemble of well-calibrated classifiers.\n",
    "The method improved the performance as expected because it grasps the wiknesses of every estimor. \n",
    "\n",
    "### Global conclusions for this second approach: \n",
    "From our interpretation of the results, linear models perform better in sentiment analysis than \"tree\" models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our final predictions are summarized in the file named results.csv and the code related to the final cross validation \n",
    "is described in the pred_final.py file. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
