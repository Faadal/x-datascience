{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/zapfack/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/zapfack/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/zapfack/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys \n",
    "sys.path.append(os.path.abspath(\"./Code\"))\n",
    "from library import *\n",
    "\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('maxent_treebank_pos_tagger') # for POS tagging\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "punct = string.punctuation.replace(\"-\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and pre-processing abstracts\n",
      "\n",
      "100 abstracts have been processed\n",
      "200 abstracts have been processed\n",
      "300 abstracts have been processed\n",
      "400 abstracts have been processed\n",
      "500 abstracts have been processed\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# read and pre-process abstracts #\n",
    "##################################\n",
    "print \"reading and pre-processing abstracts\\n\"\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "path = os.getcwd() + \"/Data/Hulth2003testing/abstracts\"\n",
    "# path = \"./Data/Hulth2003testing/abstracts\"\n",
    "os.chdir(path)\n",
    "\n",
    "# os.listdir(path)\n",
    "abstracts = []\n",
    "counter = 0\n",
    "\n",
    "for filename in sorted(os.listdir(path)):\n",
    "   f = open(filename, 'r')\n",
    "   content = f.read()\n",
    "   # remove formatting\n",
    "   content =  re.sub(\"\\s+\", \" \", content)\n",
    "   # convert to lower case\n",
    "   content = content.lower()\n",
    "   # remove punctuation (preserving intra-word dashes)\n",
    "   content = \"\".join(letter for letter in content if letter not in punct)\n",
    "   # remove dashes attached to words but that are not intra-word\n",
    "   content = re.sub(\"[^[:alnum:]['-]\", \" \", content)\n",
    "   content = re.sub(\"[^[:alnum:][-']\", \" \", content)\n",
    "   # remove extra white space\n",
    "   content = re.sub(\" +\",\" \", content)\n",
    "   # remove leading and trailing white space\n",
    "   content = content.strip()\n",
    "   # tokenize\n",
    "   tokens = content.split(\" \")\n",
    "   # remove stopwords\n",
    "   tokens_keep = [token for token in tokens if token not in stpwds]\n",
    "   # POS-tag \n",
    "   tagged_tokens = nltk.pos_tag(tokens_keep)\n",
    "   # keep only nouns and adjectives    \n",
    "   tokens_keep = [pair[0] for pair in tagged_tokens if (pair[1] in [\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"JJ\",\"JJS\",\"JJR\"])]\n",
    "   # apply Porter stemmer\n",
    "   tokens_keep = [stemmer.stem(token) for token in tokens_keep]\n",
    "   # store list of tokens\n",
    "   abstracts.append(tokens_keep)\n",
    "   \n",
    "   counter += 1\n",
    "   if counter % 100 == 0:\n",
    " \t\tprint counter, 'abstracts have been processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and pre-processing humman assigned keywords\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir(root_dir)\n",
    "\n",
    "print \"reading and pre-processing humman assigned keywords\\n\"   \n",
    "   \n",
    "path = os.getcwd() + \"/Data/Hulth2003testing/uncontr\"\n",
    "os.chdir(path)\n",
    "\n",
    "golden_keywords = []\n",
    "\n",
    "for filename in sorted(os.listdir(path)):\n",
    "   f = open(filename, 'r')\n",
    "   content = f.read()\n",
    "   # remove formatting\n",
    "   content =  re.sub(\"\\s+\", \" \", content)\n",
    "   # convert to lower case\n",
    "   content = content.lower()\n",
    "   # turn string into list of keywords, preserving intra-word dashes \n",
    "   # but breaking n-grams into unigrams to easily compute precision and recall\n",
    "   content = content.split(\";\")\n",
    "   content = [keyword.split(\" \") for keyword in content]\n",
    "   # flatten list\n",
    "   content = [keyword for sublist in content for keyword in sublist]\n",
    "   # remove empty elements\n",
    "   content = [keyword for keyword in content if len(keyword)>0]\n",
    "   # remove stopwords (rare but can happen due to n-gram breaking)\n",
    "   content = [keyword for keyword in content if keyword not in stpwds]\n",
    "   # apply Porter's stemmer\n",
    "   content = [stemmer.stem(keyword) for keyword in content]\n",
    "   # remove leading and trailing whitespace\n",
    "   content = [keyword.strip() for keyword in content]\n",
    "   # remove duplicates (can happen due to n-gram breaking)\n",
    "   content = list(set(content))\n",
    "   # save final list\n",
    "   golden_keywords.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating a gow for each abstract \n",
      "\n",
      "extracting main cores from each graph \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"creating a gow for each abstract \\n\"\n",
    "all_graphs = []\n",
    "window = 3\n",
    "for abstract in abstracts:\n",
    "    all_graphs.append(terms_to_graph(abstract, window))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting main cores from each graph \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"extracting main cores from each graph \\n\"\n",
    "mcs_weighted = []\n",
    "mcs_unweighted = []\n",
    "counter = 0\n",
    "for graph in all_graphs:\n",
    "    mcs_weighted.append(core_dec(graph, weighted = True)[\"main_core\"])\n",
    "    mcs_unweighted.append(core_dec(graph, weighted = False)[\"main_core\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average size of weighted main cores: 16\n",
      "average size of unweighted main cores: 26\n"
     ]
    }
   ],
   "source": [
    "# mcs_weighted[0].vs[\"name\"]\n",
    "# mcs_unweighted[0].vs[\"name\"]\n",
    "# len(mcs_weighted[0])\n",
    "print \"average size of weighted main cores:\", sum([len(main_core.vs[\"name\"]) \n",
    "                                                   for main_core in mcs_weighted])/len([len(main_core.vs[\"name\"]) for main_core in mcs_weighted])\n",
    "print \"average size of unweighted main cores:\",sum([len(main_core.vs[\"name\"]) \n",
    "                                                    for main_core in mcs_unweighted])/len([len(main_core.vs[\"name\"]) for main_core in mcs_unweighted])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline tf-idf \n",
      "\n",
      "4286\n"
     ]
    }
   ],
   "source": [
    "# TF_IDF\n",
    "print \"baseline tf-idf \\n\"\n",
    "\n",
    "corpus = [str(\" \".join(abstract)) for abstract in abstracts]\n",
    "\n",
    "# (optional) find out the number of unique words in abstracts\n",
    "temp = [list(set(abstract)) for abstract in abstracts]\n",
    "temptemp = [item for sublist in temp for item in sublist]\n",
    "print len(set(temptemp))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# transforms the input data into bow feature vectors with tf-idf weights\n",
    "features = vectorizer.fit_transform(corpus)\n",
    "\n",
    "colnames = vectorizer.get_feature_names()\n",
    "\n",
    "features_dense_list = features.todense().tolist()\n",
    "\n",
    "tf_idf = []\n",
    "\n",
    "for element in features_dense_list:\n",
    "    \n",
    "    # bow feature vector as list of tuples\n",
    "    row = zip(colnames,element)\n",
    "    \n",
    "    # keep only non zero values\n",
    "    # (the final length of the list should match exactly the number of unique terms in the document)\n",
    "    nonzero = [tuple for tuple in row if tuple[1]!=0]\n",
    "    \n",
    "    # rank in decreasing order\n",
    "    nonzero = sorted(nonzero, key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    # retain top 33% words as keywords\n",
    "    numb_to_retain = int(round(len(nonzero)/3))\n",
    "    \n",
    "    keywords = [tuple[0] for tuple in nonzero[:numb_to_retain]]\n",
    "    \n",
    "    tf_idf.append(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 4192)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(features_dense_list).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline PageRank\n",
      "100 graphs have been processed\n",
      "200 graphs have been processed\n",
      "300 graphs have been processed\n",
      "400 graphs have been processed\n",
      "500 graphs have been processed\n"
     ]
    }
   ],
   "source": [
    "# PageRank\n",
    "print \"baseline PageRank\"\n",
    "\n",
    "PageRank = []\n",
    "counter = 0\n",
    "\n",
    "for graph in all_graphs:\n",
    "    \n",
    "    pr_scores = graph.pagerank()\n",
    "    pr_scores = zip(graph.vs[\"name\"],pr_scores)\n",
    "    \n",
    "    # rank in decreasing order\n",
    "    pr_scores = sorted(pr_scores, key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    # retain top 33% words as keywords\n",
    "    numb_to_retain = int(round(len(pr_scores)/3))\n",
    "    \n",
    "    keywords = [tuple[0] for tuple in pr_scores[:numb_to_retain]]\n",
    "    \n",
    "    PageRank.append(keywords)    \n",
    "    \n",
    "    counter+=1\n",
    "    if counter % 100 == 0:\n",
    "        print counter, \"graphs have been processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-averaged precision: 0.60335486\n",
      "macro-averaged recall: 0.43577744\n",
      "macro-averaged F-1 score: 0.40802114\n"
     ]
    }
   ],
   "source": [
    "accuracy_mcs_weighted = []\n",
    "accuracy_mcs_unweighted = []\n",
    "accuracy_PageRank = []\n",
    "accuracy_tf_idf = []\n",
    "\n",
    "for i in range(len(golden_keywords)):\n",
    "    truth = golden_keywords[i]\n",
    "    \n",
    "    accuracy_mcs_weighted.append(accuracy_metrics(candidate = mcs_weighted[i].vs[\"name\"], truth = truth))\n",
    "    accuracy_mcs_unweighted.append(accuracy_metrics(candidate = mcs_unweighted[i].vs[\"name\"], truth = truth))\n",
    "    accuracy_PageRank.append(accuracy_metrics(candidate = PageRank[i], truth = truth))\n",
    "    accuracy_tf_idf.append(accuracy_metrics(candidate = tf_idf[i], truth = truth))\n",
    "\n",
    "# macro-averaged results (collection level)\n",
    "print \"macro-averaged precision:\", sum([tuple[0] for tuple in accuracy_mcs_weighted])/len(golden_keywords)\n",
    "print \"macro-averaged recall:\", sum([tuple[1] for tuple in accuracy_mcs_weighted])/len(golden_keywords)\n",
    "print \"macro-averaged F-1 score:\", sum([tuple[2] for tuple in accuracy_mcs_weighted])/len(golden_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44342218\n",
      "0.60200324\n",
      "0.47536026\n"
     ]
    }
   ],
   "source": [
    "print sum([tuple[0] for tuple in accuracy_mcs_unweighted])/len(golden_keywords)\n",
    "print sum([tuple[1] for tuple in accuracy_mcs_unweighted])/len(golden_keywords)\n",
    "print sum([tuple[2] for tuple in accuracy_mcs_unweighted])/len(golden_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54944962\n",
      "0.36183812\n",
      "0.41912078\n"
     ]
    }
   ],
   "source": [
    "print sum([tuple[0] for tuple in accuracy_PageRank])/len(golden_keywords)\n",
    "print sum([tuple[1] for tuple in accuracy_PageRank])/len(golden_keywords)\n",
    "print sum([tuple[2] for tuple in accuracy_PageRank])/len(golden_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58528554\n",
      "0.38595718\n",
      "0.44686596\n"
     ]
    }
   ],
   "source": [
    "print sum([tuple[0] for tuple in accuracy_tf_idf])/len(golden_keywords)\n",
    "print sum([tuple[1] for tuple in accuracy_tf_idf])/len(golden_keywords)\n",
    "print sum([tuple[2] for tuple in accuracy_tf_idf])/len(golden_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
