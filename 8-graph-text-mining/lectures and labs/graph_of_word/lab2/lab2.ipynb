{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2803, 2)\n",
      "(1396, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"./Code\"))\n",
    "from library import *\n",
    "\n",
    "##################################\n",
    "# data loading and preprocessing #\n",
    "##################################\n",
    "\n",
    "train = pd.read_csv(\"./Data/Classification/webkb-train-stemmed.txt\", header=None, delimiter=\"\\t\")\n",
    "print train.shape\n",
    "\n",
    "test = pd.read_csv(\"./Data/Classification/webkb-test-stemmed.txt\", header=None, delimiter=\"\\t\")\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first five rows of training data:\n",
      "         0                                                  1\n",
      "0  student  brian comput scienc depart univers wisconsin d...\n",
      "1  student  denni swanson web page mail pop uki offic hour...\n",
      "2  faculty  russel impagliazzo depart comput scienc engin ...\n",
      "3  student  dave phd student depart comput scienc univers ...\n",
      "4  project  center lifelong learn design univers colorado ...\n",
      "5  faculty  steve liu associ professor depart comput scien...\n",
      "first five rows of testing data:\n",
      "         0                                                  1\n",
      "0  student  eric homepag eric wei tsinghua physic fudan genet\n",
      "1   course  comput system perform evalu model new sept ass...\n",
      "2  student  home page comput scienc grad student ucsd work...\n",
      "3  student  toni web page toni face thing call toni studen...\n",
      "4   course  ec advanc comput architectur credit parallel a...\n",
      "5  faculty  faculti member ci depart research interest par...\n"
     ]
    }
   ],
   "source": [
    "# inspect head of data frames\n",
    "print \"first five rows of training data:\"\n",
    "print train.ix[:5,:]\n",
    "\n",
    "print \"first five rows of testing data:\"\n",
    "print test.ix[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing 29 documents from training set\n",
      "(2774, 2)\n",
      "removing 20 documents from testing set\n",
      "(1376, 2)\n"
     ]
    }
   ],
   "source": [
    "# get index of empty (nan) and less than four words documents\n",
    "index_remove = [i for i in range(len(train.ix[:,1])) if (train.ix[i,1]!=train.ix[i,1]) or ((train.ix[i,1]==train.ix[i,1])and(len(train.ix[i,1].split(\" \"))<4))]\n",
    "\n",
    "# remove those documents\n",
    "print \"removing\", len(index_remove), \"documents from training set\"\n",
    "train = train.drop(train.index[index_remove])\n",
    "print train.shape\n",
    "\n",
    "# repeat above steps for testing set\n",
    "index_remove = [i for i in range(len(test.ix[:,1])) if (test.ix[i,1]!=test.ix[i,1]) or ((test.ix[i,1]==test.ix[i,1])and(len(test.ix[i,1].split(\" \"))<4))]\n",
    "print \"removing\", len(index_remove), \"documents from testing set\"\n",
    "test = test.drop(test.index[index_remove])\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of observations per class:\n",
      "project : 335\n",
      "course : 620\n",
      "student : 1075\n",
      "faculty : 744\n"
     ]
    }
   ],
   "source": [
    "labels = train.ix[:,0]\n",
    "unique_labels = list(set(labels))\n",
    "\n",
    "truth = test.ix[:,0]\n",
    "unique_truth = list(set(truth))\n",
    "print \"number of observations per class:\"\n",
    "for label in unique_labels:\n",
    "    print label, \":\", len([temp for temp in labels if temp==label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storing terms from training documents as list of lists\n",
      "storing terms from testing documents as list of lists\n",
      "min, max and average number of terms per document: 4 20628 134\n"
     ]
    }
   ],
   "source": [
    "print \"storing terms from training documents as list of lists\"\n",
    "terms_by_doc = [document.split(\" \") for document in train.ix[:,1]]\n",
    "n_terms_per_doc = [len(terms) for terms in terms_by_doc]\n",
    "\n",
    "print \"storing terms from testing documents as list of lists\"\n",
    "terms_by_doc_test = [document.split(\" \") for document in test.ix[:,1]]\n",
    "\n",
    "print \"min, max and average number of terms per document:\", min(n_terms_per_doc), max(n_terms_per_doc), sum(n_terms_per_doc)/len(n_terms_per_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store all terms in list\n",
    "all_terms = [terms for sublist in terms_by_doc for terms in sublist]\n",
    "\n",
    "# compute average number of terms\n",
    "avg_len = sum(n_terms_per_doc)/len(n_terms_per_doc)\n",
    "\n",
    "# unique terms\n",
    "all_unique_terms = list(set(all_terms))\n",
    "\n",
    "# store IDF values in dictionary\n",
    "n_doc = len(labels)\n",
    "\n",
    "idf = dict(zip(all_unique_terms,[0]*len(all_unique_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 terms have been processed\n",
      "400 terms have been processed\n",
      "600 terms have been processed\n",
      "800 terms have been processed\n",
      "1000 terms have been processed\n",
      "1200 terms have been processed\n",
      "1400 terms have been processed\n",
      "1600 terms have been processed\n",
      "1800 terms have been processed\n",
      "2000 terms have been processed\n",
      "2200 terms have been processed\n",
      "2400 terms have been processed\n",
      "2600 terms have been processed\n",
      "2800 terms have been processed\n",
      "3000 terms have been processed\n",
      "3200 terms have been processed\n",
      "3400 terms have been processed\n",
      "3600 terms have been processed\n",
      "3800 terms have been processed\n",
      "4000 terms have been processed\n",
      "4200 terms have been processed\n",
      "4400 terms have been processed\n",
      "4600 terms have been processed\n",
      "4800 terms have been processed\n",
      "5000 terms have been processed\n",
      "5200 terms have been processed\n",
      "5400 terms have been processed\n",
      "5600 terms have been processed\n",
      "5800 terms have been processed\n",
      "6000 terms have been processed\n",
      "6200 terms have been processed\n",
      "6400 terms have been processed\n",
      "6600 terms have been processed\n",
      "6800 terms have been processed\n",
      "7000 terms have been processed\n",
      "7200 terms have been processed\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "counter = 0\n",
    "for element in idf.keys():\n",
    "    # number of documents in which each term appears\n",
    "    df = sum([element in terms for terms in terms_by_doc])\n",
    "    # idf\n",
    "    idf[element] = math.log10(float(n_doc+1)/df)\n",
    "    \n",
    "    counter+=1\n",
    "    if counter % 200 == 0:\n",
    "        print counter, \"terms have been processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating a graph-of-words for each training document \n",
      "\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print \"creating a graph-of-words for each training document \\n\"\n",
    "all_graphs = []\n",
    "window = 4\n",
    "for abstract in terms_by_doc:\n",
    "    all_graphs.append(terms_to_graph(abstract, window))\n",
    "    \n",
    "# sanity checks (should return True)\n",
    "print len(terms_by_doc)==len(all_graphs)\n",
    "print len(set(terms_by_doc[0]))==len(all_graphs[0].vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "computing vector representations of each training document\n",
      "100 documents have been processed\n",
      "200 documents have been processed\n",
      "300 documents have been processed\n",
      "400 documents have been processed\n",
      "500 documents have been processed\n",
      "600 documents have been processed\n",
      "700 documents have been processed\n",
      "800 documents have been processed\n",
      "900 documents have been processed\n",
      "1000 documents have been processed\n",
      "1100 documents have been processed\n",
      "1200 documents have been processed\n",
      "1300 documents have been processed\n",
      "1400 documents have been processed\n",
      "1500 documents have been processed\n",
      "1600 documents have been processed\n",
      "1700 documents have been processed\n",
      "1800 documents have been processed\n",
      "1900 documents have been processed\n",
      "2000 documents have been processed\n",
      "2100 documents have been processed\n",
      "2200 documents have been processed\n",
      "2300 documents have been processed\n",
      "2400 documents have been processed\n",
      "2500 documents have been processed\n",
      "2600 documents have been processed\n",
      "2700 documents have been processed\n"
     ]
    }
   ],
   "source": [
    "# sanity checks (should return True)\n",
    "print len(terms_by_doc)==len(all_graphs)\n",
    "print len(set(terms_by_doc[0]))==len(all_graphs[0].vs)\n",
    "\n",
    "print \"computing vector representations of each training document\"\n",
    "\n",
    "b = 0.003\n",
    "features_degree = []\n",
    "features_w_degree = []\n",
    "features_closeness = []\n",
    "features_w_closeness = []\n",
    "features_tfidf = []\n",
    "\n",
    "len_all = len(all_unique_terms)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "idf_keys = idf.keys()\n",
    "\n",
    "for i in xrange(len(all_graphs)):\n",
    "    \n",
    "    graph = all_graphs[i]\n",
    "    terms_in_doc = terms_by_doc[i]\n",
    "    doc_len = len(terms_in_doc)\n",
    "    \n",
    "    # returns node (1) name, (2) degree, (3) weighted degree, (4) closeness, (5) weighted closeness\n",
    "    metrics = compute_node_centrality(graph)\n",
    "    \n",
    "    feature_row_degree = [0]*len_all\n",
    "    feature_row_w_degree = [0]*len_all\n",
    "    feature_row_closeness = [0]*len_all\n",
    "    feature_row_w_closeness = [0]*len_all\n",
    "    feature_row_tfidf = [0]*len_all\n",
    "    \n",
    "    for term in list(set(terms_in_doc)):\n",
    "        index = all_unique_terms.index(term)\n",
    "        idf_term = idf[term]\n",
    "        denominator = (1-b+(b*(float(doc_len)/avg_len)))\n",
    "        metrics_term = [tuple[1:5] for tuple in metrics if tuple[0]==term][0]\n",
    "        \n",
    "        # store TW-IDF values\n",
    "        feature_row_degree[index] = (float(metrics_term[0])/denominator) * idf_term\n",
    "        feature_row_w_degree[index] = (float(metrics_term[1])/denominator) * idf_term\n",
    "        feature_row_closeness[index] = (float(metrics_term[2])/denominator) * idf_term\n",
    "        feature_row_w_closeness[index] = (float(metrics_term[3])/denominator) * idf_term\n",
    "        \n",
    "        # number of occurences of word in document\n",
    "        tf = terms_in_doc.count(term)\n",
    "        \n",
    "        # store TF-IDF value\n",
    "        feature_row_tfidf[index] = ((1+math.log1p(1+math.log1p(tf)))/(1-0.2+(0.2*(float(doc_len)/avg_len)))) * idf_term\n",
    "    \n",
    "    features_degree.append(feature_row_degree)\n",
    "    features_w_degree.append(feature_row_w_degree)\n",
    "    features_closeness.append(feature_row_closeness)\n",
    "    features_w_closeness.append(feature_row_w_closeness)\n",
    "    features_tfidf.append(feature_row_tfidf)\n",
    "\n",
    "    counter += 1\n",
    "    if counter % 100 == 0:\n",
    "        print counter, \"documents have been processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
